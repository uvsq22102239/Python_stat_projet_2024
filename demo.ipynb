{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9083b3f-c59d-4cd7-b967-72e573054e04",
   "metadata": {},
   "source": [
    "<h1><center>Équipe F</center></h1>\n",
    "<h1>M1-ISDD</h1>\n",
    "AUCLAIR LUCAS<br>\n",
    "COUSTILLAS LAURÉDANE<br>\n",
    "ZOHDY-CAUVIN SALMA<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de620315-2b75-4e82-9a07-47eb2b829298",
   "metadata": {},
   "source": [
    "# Régression linéaire et ACP avec statsmodels et scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f22dd7-0025-4a69-9813-96f7fe014ed8",
   "metadata": {},
   "source": [
    "- Ce projet a été réalisé dans le cadre de l'UE Python2 du Master de Bioinformatique de l'Université Paris Cité.\n",
    "- Le but de celui-ci est de réaliser une régression linéaire et une analyse ACP avec les bibliothèques statsmodels et scikit-learn ainsi que de comparer leurs utilisations.\n",
    "- Le projet doit être un tutoriel, ce faisant, une vidéo tutoriel complémentaire a été réalisée en parallèle.\n",
    "- Le présent Notebook existe pour réaliser celui-ci et présenter les données le plus clairement possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa4012-a4ee-44f5-bfa5-4c5e32e665f5",
   "metadata": {},
   "source": [
    "## La bibliothèque scikit-learn :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fefe6-09b1-4c00-9044-ef2bdf0f1967",
   "metadata": {},
   "source": [
    "- Création : développée en 2007 par David Cournapeau dans le cadre du programme *Google Summer of Code*.\n",
    "- La première version publique (v0.1 beta) est sortie en Janvier 2010 avec l'aide de l'INRIA (Institut National de Recherche en Informatique et en Automatique).\n",
    "- Dernière version : 1.5.2 (11 Septembre, 2024).\n",
    "- Pour installer dans un environnement conda :\n",
    "```bash\n",
    "$ conda install scikit-learn\n",
    "```\n",
    "Pour la version spécifique :\n",
    "```bash\n",
    "$ conda install scikit-learn=1.5.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec222be-40da-4912-8029-95df16d1c476",
   "metadata": {},
   "source": [
    "## La bibliothèque statsmodels  :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9349e4-df75-462d-9062-a399bbe63e40",
   "metadata": {},
   "source": [
    "- Création : Par Skipper Seabold et Josef Perktold en 2009 dans le cadre du programme *Google Summer of Code*.\n",
    "- Dernière version : 0.14.4 (Octobre 2024)\n",
    "- Pour installer dans un environnement conda :\n",
    "```bash\n",
    "$ conda install statsmodels\n",
    "```\n",
    "Pour la version spécifique :\n",
    "```bash\n",
    "$ conda install statsmodels=0.14.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1f6a5-94a0-463c-9862-ac3f5a8e0849",
   "metadata": {},
   "source": [
    "## Points en commun entre les deux bibliothèques :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4639ba-9124-4c24-9b22-338a551021ed",
   "metadata": {},
   "source": [
    "- Ce sont des outils importants pour l'analyse de données.\n",
    "- Elles sont utilisées pour des modèles de statistiques (tests, régression linéaire) et de machine learning (notamment Scikit-learn).\n",
    "- Elles sont compatibles avec d'autres bibliothèques usuelles en Python comme Numpy, Pandas et Matplotlib.\n",
    "- Elles peuvent faire des ACP (Analyse en Composantes Principales, PCA en anglais)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bd6bd-e19f-4298-8a1b-1e4ef0101b14",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a91ef-1730-4923-b052-acb5560099d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la gestion du jeu de données.\n",
    "import pandas as pd\n",
    "# Pour les représentations graphiques.\n",
    "import matplotlib.pyplot as plt\n",
    "# Pour l'analyse statistique.\n",
    "# .api est l'importation classique.\n",
    "import statsmodels.api as sm\n",
    "import sklearn as sk\n",
    "# sous-bibliothèque essentielle.\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ac5f7-79f7-4c2a-8c02-84d521a5bf08",
   "metadata": {},
   "source": [
    "- Fixer la racine utilisée pour les nombres aléatoires afin d'éviter certaines variations mineures qui ont lieu lorsque le code est exécuté plusieurs fois ou lorsqu'il est exécuté sur différentes machines.\n",
    "- Les méthodes utilisées pour l'analyse statistique sont déterministes et ne devraient pas varier normalement (ou très peu). Néanmoins, parfois, certaines différences ont pu être constatées, ceci permet donc d'améliorer la reproductibilité des données générées par le code et les analyses dans le notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c20919-0ca8-4792-8210-823e16d2e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèque importée pour définir la racine.\n",
    "import numpy as np\n",
    "# Définition de la racine.\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b768a-3d38-41fe-b103-ca58e0d22e96",
   "metadata": {},
   "source": [
    "- Jupyter notebook magic command pour afficher ce que le code print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf6a721-2eb3-4fe7-afe2-3130d2b12f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c1b9c-52f3-41c7-b24a-bd1f902e9f25",
   "metadata": {},
   "source": [
    "## Importation des données **penguins_raw.csv** dans un Dataframe pour l'analyse statistique :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87468f1-7afa-47b1-9693-042ab36342db",
   "metadata": {},
   "source": [
    "- Le fichier penguins_raw.csv contient des données écologiques collectées sur des manchots de trois espèces différentes : Adélie, Chinstrap, et Gentoo, originaires de l'archipel des îles Palmer en Antarctique.\\\n",
    "Ce jeu de données est une ressource populaire pour l'apprentissage des analyses statistiques et la visualisation de données. Il s'agit d'une alternative moderne et accessible au jeu de données historique \"Iris\" et illustre des concepts de biologie, d'écologie et d'analyse de données.\n",
    "- Les données brutes ont été prises à la place des données simplifiées afin de réaliser une analyse de données et des comparaisons pertinentes (données simplifiées : penguins.csv)\\\n",
    "La colonne **Individual ID** est prise comme colonne des indices pour le Dataframe du jeu de données comme tous les IDs sont différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144c325-e069-47a9-b159-2d643f3401d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"penguins_raw.csv\", index_col=\"Individual ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb7649-9456-4fa4-a37c-c3cb659b959d",
   "metadata": {},
   "source": [
    "- Taille du Dataframe sous la forme (**lignes**, **colonnes**) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c004f-6893-46e0-bcc0-48e5afea15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394a19c-ec32-40cf-98b9-ad7b65281b41",
   "metadata": {},
   "source": [
    "- Vérifier si des valeurs Null sont présentes dans chaque colonne du jeu de données (*i.e* NaN or NA) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86300c63-19b2-48f2-9f67-44e80a1b25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1f211-6a01-49b0-b634-ad1a07230095",
   "metadata": {},
   "source": [
    "- Comptabiliser les valeurs Null pour chaque colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe66e3-5e4a-41a2-866c-7ae6229d4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931aa4b7-4303-4665-a591-588caf06cacf",
   "metadata": {},
   "source": [
    "- Regarder globalement à quoi ressemble le jeu de données : *\"head\"* pour les premières lignes du jeu de données et *\"tail\"* pour les dernières lignes de celui-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6192e-f32b-41a1-b8c2-ae90cb5948b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4ebfd-22f7-4313-b2f6-3206bd8013b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5d8be-6113-4205-89ad-152ace605d8d",
   "metadata": {},
   "source": [
    "- On supprime la colonne **Comments** car elle ne servira pas à l'analyse. En effet, c'est celle qui contient le plus de valeurs Null, elle n'est donc pas pertinente pour notre analyse de données.\n",
    "- Par ailleurs, les commentaires en question ont l'air de plus se référer à des critiques des observations, leur absence n'est donc pas un défaut mais plutôt une preuve que l'échantillon devrait être correct.\n",
    "- De plus, cela évite d'enlever 290 lignes de notre jeu de données et permet donc d'éviter les biais induits par la diminution de la taille de celui-ci.\\\n",
    "**N.B.** : *axis = 0* pour les lignes et *axis = 1* pour les colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede547d-81d2-4798-b294-857648165e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Comments', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bdbc92-7b7c-4d15-bdaa-c39322eebfb0",
   "metadata": {},
   "source": [
    "- On supprime toutes les lignes comportant des valeurs Null restantes (comme indiqué dans les consignes), la modification est faite **inplace/sur place** car nous n'avons pas besoin d'un sous-ensemble du jeu de données avec des valeurs Null.\n",
    "- En ayant enlevé la colonne **Comments** au préalable, cela permet d'enlever moins de 30 lignes au total, là où sinon notre jeu de données à analyser serait complètement vide ou presque, ne l'ayant pas fait.\n",
    "- **N.B. : Il est important de minimiser la diminution du nombre de variables dans le jeu de données pour une analyse pertinente**, ce faisant, il faut savoir quelles variables nous intéressent et quelle quantité de ces variables nous est nécessaire dans notre analyse. Ici, en enlevant toutes les valeurs Null après avoir enlevé la colonne **Comments**, on ne perd qu'au maximum 14 variables. Ce faisant, on considère qu'on peut garder les autres colonnes ou éviter d'utiliser d'autres méthodes plus complexes pour gérer un jeu de données contenant de nombreuses valeurs Null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b911f-81d5-40e9-bb35-4d99a8c99aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eafedc-409d-47ab-83a4-8252a16fd5e7",
   "metadata": {},
   "source": [
    "- On vérifie que les modifications faites sont correctes et correspondent à nos attentes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bffccc-3597-4ab4-a229-1883476a074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b1bfa-5d8b-467b-b751-6ba4cfc8881d",
   "metadata": {},
   "source": [
    "- On observe que tout s'est déroulé comme souhaité préalablement : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d669b7a-0275-44fa-9473-338bb7a60f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e8b21-b823-4a6a-aa64-b5a66f0074e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62de02-59ac-4966-9210-bb80561ed067",
   "metadata": {},
   "source": [
    "## Principes de la régression linéaire :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02bed8-4e89-4ebb-a982-8ba89ddd50f5",
   "metadata": {},
   "source": [
    "### Définitions :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4ff12-5e2f-40f6-b713-bf0a2f878f00",
   "metadata": {},
   "source": [
    "- En mathématiques et notamment en statistiques, une régression est un ensemble de méthodes différentes permettant d'approcher une variable à partir d'autres qui lui sont corrélées, le tout en créant un modèle mathématique qui approche le plus possible la variable étudiée et celles qui lui sont corrélées, ce qui permet à terme d'étudier les relations existantes (ou non) entre ces variables.<br><br>\n",
    "- Par extension, un modèle de régression linéaire est un modèle qui tend à établir une relation linéaire entre une variable, dite *expliquée*, et une ou plusieurs variables, dites *explicatives*, souvent dans le but de prédire la variable *expliquée* à partir des variables *explicatives*.<br><br>\n",
    "- **N.B.** : La régression linéaire repose sur l'algèbre linéaire et donc par extension par exemple la diagonalisation de matrices de covariance. Cela n'est absolument pas abordé ici comme le but est de créer un tutoriel abordable et facilement compréhensible pour tous et non de faire un cours d'algèbre linéaire.<br><br>\n",
    "- **Pour plus d'informations :**\n",
    "    - [Page Wikipedia Régression linéaire](https://fr.wikipedia.org/wiki/R%C3%A9gression_lin%C3%A9aire)\n",
    "    - [Page Wikipedia Régression (statistiques)](https://fr.wikipedia.org/wiki/R%C3%A9gression_(statistiques))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538518b-3df5-4db1-9137-97869edaa29d",
   "metadata": {},
   "source": [
    "### Hypothèses et propriétés de bases :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb9afc-6b65-4135-aae5-4516b8627c0a",
   "metadata": {},
   "source": [
    "- Soit *Xi* les variables explicatives et *Y* la variable expliquée :\n",
    "    - La régression linéaire repose sur l'hypothèse qu'il existe une relation linéaire entre la variable *expliquée Y* et les variables *explicatives Xi*.\n",
    "    - Dans sa forme la plus simple, cette relation est décrite par une droite : *Y = aXi + b + ϵ*,\\\n",
    "où *a* est la pente de la droite, *b* l'ordonnée à l'origine, et *ϵ* un terme d'erreur représentant les écarts aléatoires par rapport à la droite.<br><br>\n",
    "- La régression linéaire cherche à trouver les valeurs des paramètres *a* et *b* qui minimisent l'écart entre les prédictions du modèle et les valeurs observées de *Y*.\n",
    "Cela se fait généralement en minimisant la somme des carrés des erreurs.\\\n",
    "Exemple de méthode des moindres carrés avec OLS (Ordinary Least Squares regression) :\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^{n} (Y_{\\text{observé}, i} - Y_{\\text{prédit}, i})^2\n",
    "$$\n",
    "    - où :\n",
    "        - *Yobservé* : est la valeur observée réelle de la variable expliquée pour le *i*-ème point de données,\n",
    "        - *Yprédit* : est la valeur prédite de la variable expliquée pour le *i*-ème point de données,\n",
    "        - *n* : est le nombre total d'observations.<br><br>\n",
    "- Hypothèses à vérifier pour que le modèle soit significatif et pertinent:\n",
    "    - Non colinéarité des variables explicatives\n",
    "    - Indépendance des erreurs\n",
    "    - Exogénéité\n",
    "    - Homoscédasticité\n",
    "    - Normalité des termes d'erreur<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd969b-2e34-4897-8efa-af321b4e0fe0",
   "metadata": {},
   "source": [
    "## Réalisation d'une régression linéaire avec les deux bibliothèques :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e30b5-f78d-4e64-bab6-980b56054177",
   "metadata": {},
   "source": [
    "- On va choisir trois variables quantitatives pertinentes et réaliser une régression linéaire avec les deux bibliothèques, entre les variables deux par deux (la première vs la deuxième, la deuxième vs la troisième, la troisième vs la première) et représenter les résultats graphiquement.<br><br>\n",
    "- On choisit les variables suivantes pour des pertinences biologiques et statistiques :\n",
    "    - Culmen Length (mm) soit la longueur du culmen (bec)\n",
    "    - Flipper Length (mm) soit la longueur des nageoires\n",
    "    - Body Mass (g) soit la masse corporelle.<br><br>\n",
    "- En effet, ces trois variables englobent des indicateurs clés sur l'adaptation et le comportement des pingouins et semblent être corrélées. Par exemple, les pingouins plus grands, donc qui ont une longueur de bec et de nageoire plus grande, ont tendance à avoir une masse corporelle plus élevée. Ainsi, ces variables présentent une forte variance au sein des données et sont donc pertinentes pour une analyse statistique. Par ailleurs, dans le contexte actuel du changement climatique et des catastrophes naturelles, il est intéressant de regarder les relations entre les caractères morphologiques des pingouins, notamment pour mieux comprendre comment ceux-ci s'adaptent à leur environnement dans un contexte écologique et de conservation des espèces.<br><br>\n",
    "- Aucune variable commune entre les deux exemples ne sera utilisée pour bien expliciter la manière d'utiliser ces deux bibliothèques de manière claire et limpide (*i.e.* les variables utilisées seront redéfinies à chaque fois)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98533d-1989-4bb1-a09e-4c793f0017b6",
   "metadata": {},
   "source": [
    "### Avec scikit-learn, bibliothèque plutôt utilisée pour le machine learning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb460e-143a-4936-82bf-354dde6364c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des variables quantitatives pour les modèles de régression.\n",
    "# y1 représente la longueur du bec et x1 la longueur de la nageoire.\n",
    "y1 = df['Culmen Length (mm)']\n",
    "x1 = df['Flipper Length (mm)']\n",
    "\n",
    "# y2 représente la longueur de la nageoire et x2 la masse corporelle.\n",
    "y2 = df['Flipper Length (mm)']\n",
    "x2 = df['Body Mass (g)']\n",
    "\n",
    "# y3 représente la masse corporelle et x3 la longueur du bec.\n",
    "y3 = df['Body Mass (g)']\n",
    "x3 = df['Culmen Length (mm)']\n",
    "\n",
    "\n",
    "# Création d'une fonction pour tracer les données et la ligne de régression.\n",
    "def plot_regression(x, y, model, x_label, y_label):\n",
    "    \"\"\"\n",
    "    Trace les données observées et la ligne de régression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pd.Series\n",
    "        La variable explicative.\n",
    "    y : pd.Series\n",
    "        La variable expliquée.\n",
    "    model : sk.linear_model.LinearRegression\n",
    "        Le modèle de régression ajusté.\n",
    "    x_label : str\n",
    "        L'étiquette pour l'axe des x.\n",
    "    y_label : str\n",
    "        L'étiquette pour l'axe des y.\n",
    "    \"\"\"\n",
    "    # Création d'une figure pour le tracé.\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Tracer un nuage de points pour les données observées en bleu.\n",
    "    plt.scatter(x, y, color='pink', label='Données observées')\n",
    "    # Tracer la ligne de régression en rouge.\n",
    "    # Utilise .reshape(-1, 1) car le modèle attend un tableau 2D pour x.\n",
    "    # .predict() prédit les valeurs de y en fonction de x.\n",
    "    plt.plot(\n",
    "        x,\n",
    "        model.predict(x.values.reshape(-1, 1)),\n",
    "        color='blue',\n",
    "        label='Ligne de régression'\n",
    "    )\n",
    "    # Ajouter des étiquettes pour les axes x et y.\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    # Ajouter une légende et une grille au graphique.\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    # Ajouter un titre au graphique.\n",
    "    plt.title(f\"Ligne de régression : {y_label} vs {x_label}\")\n",
    "    # Afficher le graphique.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Création et ajustement du modèle 1 pour les variables x1 et y1.\n",
    "model1 = sk.linear_model.LinearRegression()\n",
    "x1_reshaped = x1.values.reshape(-1, 1)  # Mise en forme en 2D pour le modèle.\n",
    "model1.fit(x1_reshaped, y1)  # Ajuste le modèle aux données.\n",
    "\n",
    "# Création et ajustement du modèle 2 pour les variables x2 et y2.\n",
    "model2 = sk.linear_model.LinearRegression()\n",
    "x2_reshaped = x2.values.reshape(-1, 1)  # Mise en forme en 2D pour le modèle.\n",
    "model2.fit(x2_reshaped, y2)  # Ajuste le modèle aux données.\n",
    "\n",
    "# Création et ajustement du modèle 3 pour les variables x3 et y3.\n",
    "model3 = sk.linear_model.LinearRegression()\n",
    "x3_reshaped = x3.values.reshape(-1, 1)  # Mise en forme en 2D pour le modèle.\n",
    "model3.fit(x3_reshaped, y3)  # Ajuste le modèle aux données.\n",
    "\n",
    "# Affichage des résultats pour chaque modèle.\n",
    "# (coefficient, intercept, MSE et R²).\n",
    "for i, (model, x, y) in enumerate(\n",
    "    [(model1, x1, y1),\n",
    "     (model2, x2, y2),\n",
    "     (model3, x3, y3)], start=1):\n",
    "    # Calcul de l'erreur quadratique moyenne (MSE)\n",
    "    # et du R² pour chaque modèle.\n",
    "    mse = sk.metrics.mean_squared_error(\n",
    "        y, model.predict(x.values.reshape(-1, 1)))\n",
    "    r_squared = sk.metrics.r2_score(\n",
    "        y, model.predict(x.values.reshape(-1, 1)))\n",
    "    # Affiche les résultats du modèle : coefficient, intercept, MSE et R².\n",
    "    print(f\"\\nModèle {i} :\")\n",
    "    print(f\"Coefficient de régression : {model.coef_[0]}\")\n",
    "    print(f\"Ordonnée à l'origine : {model.intercept_}\")\n",
    "    print(f\"MSE : {mse}\\nR²  : {r_squared}\")\n",
    "\n",
    "# Tracer des résultats pour chaque modèle avec des étiquettes d'axes.\n",
    "plot_regression(\n",
    "    x=x1, y=y1, model=model1,\n",
    "    x_label='Longueur de la nageoire (mm)',\n",
    "    y_label='Longueur du bec (mm)')\n",
    "\n",
    "plot_regression(\n",
    "    x=x2, y=y2, model=model2,\n",
    "    x_label='Masse corporelle (g)',\n",
    "    y_label='Longueur de la nageoire (mm)')\n",
    "\n",
    "plot_regression(\n",
    "    x=x3, y=y3, model=model3,\n",
    "    x_label='Longueur du bec (mm)',\n",
    "    y_label='Masse corporelle (g)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a142f-ace5-4a36-a7ad-04ba86e13bca",
   "metadata": {},
   "source": [
    "### Avec statmodels, bibliothèque plutôt utilisée pour les études statistiques classiques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e398bde-1187-4fa4-b018-d1c62b73acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction pour effectuer une régression linéaire.\n",
    "def perform_regression(x, y):\n",
    "    \"\"\"\n",
    "    Effectue une régression linéaire entre deux variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pd.Series\n",
    "        La variable explicative.\n",
    "    y : pd.Series\n",
    "        La variable expliquée.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "        Résultats du modèle de régression linéaire ajusté.\n",
    "    \"\"\"\n",
    "    # Ajouter une constante pour inclure\n",
    "    # une ordonnée à l'origine (interception) dans le modèle.\n",
    "    X = sm.add_constant(x)\n",
    "    # Ajuster un modèle de régression linéaire en utilisant\n",
    "    # la méthode des moindres carrés (Ordinary Least Squares - OLS)\n",
    "    # .fit() ajuste le modèle pour y et X afin de minimiser\n",
    "    # les erreurs.\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    # Retourne l'objet contenant les résultats du modèle ajusté.\n",
    "    return model\n",
    "\n",
    "\n",
    "# Création d'une fonction pour visualiser la régression avec un graphique.\n",
    "def plot_regression(x, y, model, x_label, y_label):\n",
    "    \"\"\"\n",
    "    Trace les données observées et la ligne de régression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pd.Series\n",
    "        La variable explicative.\n",
    "    y : pd.Series\n",
    "        La variable expliquée.\n",
    "    model : statsmodels.regression.linear_model.RegressionResultsWrapper\n",
    "        Le modèle de régression ajusté.\n",
    "    x_label : str\n",
    "        L'étiquette pour l'axe des x.\n",
    "    y_label : str\n",
    "        L'étiquette pour l'axe des y.\n",
    "    \"\"\"\n",
    "    # Créer une figure pour le graphique de régression\n",
    "    # avec une taille spécifiée.\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Tracer les données en tant que nuage de points en bleu.\n",
    "    plt.scatter(x, y, color='pink', label='Données observées')\n",
    "    # Tracer la ligne de régression en rouge,\n",
    "    # avec les prédictions du modèle ajusté.\n",
    "    # .predict() prédit les valeurs de y en fonction de x.\n",
    "    plt.plot(\n",
    "        x,\n",
    "        model.predict(sm.add_constant(x)),\n",
    "        color='blue',\n",
    "        label='Ligne de régression'\n",
    "    )\n",
    "    # Ajouter des étiquettes aux axes x et y.\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    # Titre du graphique qui précise les variables étudiées.\n",
    "    plt.title(f'Ligne de régression : {y_label} vs {x_label}')\n",
    "    # Ajouter une légende pour expliquer les éléments du graphique.\n",
    "    plt.legend()\n",
    "    # Ajouter une grille au graphique.\n",
    "    plt.grid()\n",
    "    # Afficher le graphique.\n",
    "    plt.show()\n",
    "\n",
    "# Sélectionner des variables quantitatives.\n",
    "# Variables pour la première régression.\n",
    "\n",
    "\n",
    "y1 = df['Culmen Length (mm)']  # Longueur du bec en mm.\n",
    "x1 = df['Flipper Length (mm)']  # Longueur de la nageoire en mm.\n",
    "\n",
    "# Variables pour la deuxième régression.\n",
    "y2 = df['Flipper Length (mm)']  # Longueur de la nageoire en mm.\n",
    "x2 = df['Body Mass (g)']        # Masse corporelle en grammes.\n",
    "\n",
    "# Variables pour la troisième régression.\n",
    "y3 = df['Body Mass (g)']         # Masse corporelle en grammes.\n",
    "x3 = df['Culmen Length (mm)']    # Longueur du bec en mm.\n",
    "\n",
    "# Effectuer les régressions linéaires\n",
    "# pour chaque paire de variables.\n",
    "model1 = perform_regression(x1, y1)  # Première régression.\n",
    "model2 = perform_regression(x2, y2)  # Deuxième régression.\n",
    "model3 = perform_regression(x3, y3)  # Troisième régression.\n",
    "\n",
    "# Afficher les résumés des modèles et l'erreur quadratique moyenne (MSE)\n",
    "# pour chaque régression.\n",
    "print(\"\\nModèle 1 Résumé :\")\n",
    "# Résumé des statistiques du modèle de la première régression.\n",
    "print(model1.summary())\n",
    "# Erreur quadratique moyenne (MSE) pour la première régression.\n",
    "print(\"MSE :\", model1.mse_total)\n",
    "\n",
    "print(\"\\nModèle 2 Résumé :\")\n",
    "# Résumé des statistiques du modèle de la deuxième régression.\n",
    "print(model2.summary())\n",
    "# Erreur quadratique moyenne (MSE) pour la deuxième régression.\n",
    "print(\"MSE :\", model2.mse_total)\n",
    "\n",
    "print(\"\\nModèle 3 Résumé :\")\n",
    "# Résumé des statistiques du modèle de la troisième régression.\n",
    "print(model3.summary())\n",
    "# Erreur quadratique moyenne (MSE) pour la troisième régression.\n",
    "print(\"MSE :\", model3.mse_total)\n",
    "\n",
    "# Tracer les résultats de chaque régression.\n",
    "plot_regression(\n",
    "    x1, y1, model1,\n",
    "    'Longueur de la nageoire (mm)',  # Étiquette pour l'axe x\n",
    "    'Longueur du bec (mm)'           # Étiquette pour l'axe y\n",
    ")\n",
    "plot_regression(\n",
    "    x2, y2, model2,\n",
    "    'Masse corporelle (g)',          # Étiquette pour l'axe x\n",
    "    'Longueur de la nageoire (mm)'   # Étiquette pour l'axe y\n",
    ")\n",
    "plot_regression(\n",
    "    x3, y3, model3,\n",
    "    'Longueur du bec (mm)',          # Étiquette pour l'axe x\n",
    "    'Masse corporelle (g)'           # Étiquette pour l'axe y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0929e-b0e1-4635-bef9-19e103a581d1",
   "metadata": {},
   "source": [
    "### Comparaison entre les deux :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f48bf2-c0e9-4e26-925c-c0d9b1a8bc79",
   "metadata": {},
   "source": [
    "- On peut d'ores et déjà remarquer que statmodels est plus flexible que scikit-learn. En effet, scikit-learn nécessite des tableaux 2D (matrice ou array) tandis que statmodels ne requiert aucun formatage particulier de la sorte. Cela est dû au fait que pour ses capacités de machine learning, scikit-learn a besoin d'être efficace et de pouvoir gérer de grands ensembles de données efficacement et automatiquement ainsi que de gérer de nombreuses opérations de scaling. Ce formatage permet également un prétraitement uniforme des données facilité (normalisation, standardisation, ...).<br><br>\n",
    "- statmodels semble plus clair au niveau interprétabilité, à directement et facilement afficher le résumé de l'analyse ainsi que ses descripteurs usuels pour l'évaluation de celle-ci (critères d'évaluation). scikit-learn ne le fait pas aussi facilement, car dans des contextes de machine learning l'utilisateur doit pouvoir évaluer son modèle selon des besoins bien spécifiques.<br><br>\n",
    "- On remarque que statmodels fournit une analyse plus approfondie des résultats tels que des intervalles de confiance (ressemble à R dans le formatage), même si cela est aussi possible avec scikit-learn quoique plus sommaire et non directement disponible. Par contre, on remarque que statmodels indique si le modèle obtenu présente des erreurs potentielles, comme vu dans les messages affichés par le résumé, afin d'affiner les modèles obtenus et de permettre une analyse statistique des données complète.<br><br>\n",
    "- On observe une adéquation parfaite à l'œil  nu entre les représentations graphiques des deux modèles. Regardons maintenant les descripteurs pour l'évaluation de la qualité du modèle afin d'affiner notre comparaison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eacbff-9019-4eac-a7fa-56294240ca91",
   "metadata": {},
   "source": [
    "## Critères d'évaluations basiques importants et détails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9d54a-9a0c-4ec9-9bbe-904cad326f2d",
   "metadata": {},
   "source": [
    "- **R² (coefficient de détermination) :**\n",
    "    - Il mesure la proportion de la variance de la variable dépendante expliquée par le modèle. Son score varie entre 0 et 1, où une valeur proche de 1 indique que le modèle explique bien les variations des données, tandis qu’une valeur proche de 0 suggère que le modèle explique peu de variabilité. \"R-squared\" sous statmodels dans le summary et \".r2_score()\" (dans le code, sinon affiché comme \"R²\" avec les print) sous scikit.\n",
    "    $$\n",
    "R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "où :\n",
    "  - **SSE** est la somme des carrés des erreurs, donnée par :\n",
    "  \n",
    "    $$\n",
    "    \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "    $$\n",
    "  \n",
    "  - **SST** est la somme des carrés totaux par rapport à la moyenne des valeurs observées :\n",
    "  \n",
    "    $$\n",
    "    \\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "    $$\n",
    "\n",
    "- **MSE** (erreur quadratique moyenne) :\n",
    "    - Elle évalue l’erreur moyenne de prédiction du modèle, en prenant la moyenne des carrés des erreurs entre les valeurs observées et prédites. Plus le MSE est faible (proche de 0), plus le modèle est précis. \".mse_total\" sous statmodels (dans le code,  sinon affiché comme \"MSE\" avec les print)  et \"mean_squared_error()\" (dans le code, sinon affiché comme \"MSE\" avec les print) sous scikit.\n",
    "    $$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "où :\n",
    "  - *yi* : est la valeur observée,\n",
    "  - *ŷi* : est la valeur prédite par le modèle,\n",
    "  - *n* : est le nombre de données.<br><br>\n",
    "\n",
    "- **Coefficient de régression (pente de la courbe) :**\n",
    "  - Le coefficient (ou pente) indique l'effet du changement de la variable explicative *Xi* sur la variable expliquée *Y*. Un coefficient positif signifie que lorsque *Xi* augmente, *Y* tend également à augmenter, tandis qu'un coefficient négatif signifie que *Y* diminue lorsque *Xi* augmente. \"coef\" sous statmodels dans le summary et \"model.coef_\" (dans le code sinon affiché comme \"Coefficient de régression\" avec les print) sous scikit.<br><br>\n",
    "\n",
    "- **Ordonnée à l'origine :**\n",
    "  - L'ordonnée à l’origine représente la valeur de *Y* lorsque *Xi* = 0. Cette valeur peut être interprétée comme la base à partir de laquelle *Y* évolue en fonction de *Xi*. Son interprétation est dépendante du contexte, elle peut ne pas être très pertinente en soi. Premier paramètre de \".params\" dans statmodels (n'a pas été affiché par souci de lisibilité) et \"model.intercept\" (dans le code, sinon affiché comme \"Ordonnée à l'origine\" avec les print) sous scikit.<br><br>\n",
    " \n",
    "- **Brève conclusion vis-à-vis de la régression linéaire réalisée :**\n",
    "    - **Tous les R² sont >0, indiquant donc une corrélation entre chaque paire de variables. Celle-ci est plus prononcée (car plus proche de 1, 0.7) pour la seconde paire de variables \"Flipper Length (mm) vs Body Mass (g)\".**<br><br>\n",
    "    - **Néanmoins, on observe que les MSE sont relativement voire très élevées** (654 930 pour la troisième paire de variables), cela pourrait être acceptable si les valeurs étaient très élevées elles aussi mais ce n'est pas le cas. La plus petite MSE est de 17 pour la première paire de variables ce qui pourrait être acceptable si on considère que c'est proche de 0...<br><br>\n",
    "    - **En somme, la régression linéaire montre des corrélations prononcées entre les variables, notamment \"Flipper Length (mm) vs Body Mass (g)\", mais la proximité des prédictions avec la réalité semble fortement compromise par les valeurs élevées des MSE.<br><br>**\n",
    "    - **Lien avec les signalements de statmodels, multicolinéarité et le fait que les valeurs ne sont ni centrées ni réduites (non normalisées) : en somme problème d'échelles --> voir la suite.<br><br>**\n",
    "- **Pour plus d'informations sur les autres descripteurs affichés :**\n",
    "    - [Doc. statmodels](https://www.statsmodels.org/devel/index.html)\n",
    "    - [Le Summary de statmodels et autres](https://blog.dailydoseofds.com/p/statsmodel-regression-summary-will)\n",
    "    - [Doc. scikit](https://scikit-learn.org/stable/)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ab0cb-25f0-4029-9b76-aab299f8a074",
   "metadata": {},
   "source": [
    "- **Détails :**\n",
    "    - Les R² obtenus sont les mêmes, ce qui montre bien que les deux bibliothèques sont aussi efficaces. Néanmoins, ce n'est pas le cas pour les MSE mais cela est sûrement dû à une différence dans leur définition et/ou affichage entre les deux bibliothèques. Elles pourraient ne pas être comparables sur ce point là. Mais comme les MSE calculées par scikit sont plus faibles à chaque fois que celles calculées par statmodels (même si du même ordre de grandeur), on peut aussi dire que scikit affine davantage la prédiction des résultats à la réalité que statmodels (différence entre machine learning et statistiques classiques). On remarque que le modèle 2 explique le plus de variance, même si comme le dit la seconde note du résumé (comme pour le modèle 1) de statmodels, il semble y avoir un problème de multicolinéarité des variables.<br><br>\n",
    "    - La multicolinéarité se produit lorsque deux ou plusieurs variables explicatives dans un modèle de régression sont fortement corrélées entre elles. Cela est problématique, notamment parce qu’elle rend l’estimation des coefficients de régression instable et réduit donc la fiabilité des résultats. En effet, car cela empêche le modèle de déterminer l'influence indépendante de chaque variable explicative.<br><br>\n",
    "    - La multicolinéarité complique par conséquent l’interprétation des coefficients. Normalement, un coefficient dans une régression linéaire représente l’effet marginal de la variable explicative sur la variable dépendante, toutes les autres variables étant maintenues constantes. Mais lorsque deux variables sont très corrélées, cet effet marginal perd de son sens car il est difficile de faire varier une variable sans affecter l’autre.<br><br>\n",
    "    - La première note du résumé de statmodels indique que les erreurs-types calculées reposent sur l’hypothèse que la matrice de covariance des erreurs est correctement spécifiée. Si le modèle est un premier test ce n'est pas un problème majeur, mais il est effectivement  important d’en tenir compte car une matrice mal spécifiée peut entraîner des erreurs d'interprétation des coefficients.<br><br>\n",
    "    - Ce faisant, on rejoint donc la différence entre statmodels et scikit-learn au niveau de la flexibilité : scikit-learn est moins flexible et impose un tableau 2D mais cela est fait pour optimiser ses performances, là où statmodels attend que l'utilisateur sache ce qu'il fait mais affiche en plus des conseils et signalements si certaines conditions ne sont pas respectées (*i.e*  hypothèses à vérifier pour que le modèle soit significatif et pertinent).<br><br>\n",
    "    - **Dans notre cas, le signalement automatique 1 de statmodels est pertinent car nous n'avons pas vérifié les conditions de validité comme le but du projet est de présenter les deux librairies et non de faire une étude statistique poussée et exhaustive avec hypothèses à l'appui. Le signalement 2 montre qu'il y'a un problème au niveau de l'échelle des données, les données ont donc peut-être une trop grande variabilité de base. On remarque que nous n'avons à aucun moment centré et/ou réduit les données (*i.e.* normalisé) ou utilisé un z-score ce qui pose nécessairement problème comme on compare des valeurs en \"mm\" à des valeurs en \"g\", causant un problème d'échelle et donc sûrement le signalement 2.<br><br>**\n",
    "    - **Formule du z-score (normalisation standard):**\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "où :\n",
    "- z : Le z-score (valeur normalisée)\n",
    "- x : La valeur initiale\n",
    "- $\\mu$ : La moyenne de l'ensemble des données\n",
    "- $\\sigma$ : L'écart-type de l'ensemble des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e32107-98f9-4045-ae80-16f9f31eaa8a",
   "metadata": {},
   "source": [
    "#### Modèle 2 corrigé, soit *Flipper Length (mm)* vs *Body Mass (g)* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c827526-b78f-4d44-be84-b6640bd371d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Flipper Length (mm)']\n",
    "x = df['Body Mass (g)']\n",
    "\n",
    "# Standardisation des données.\n",
    "x_scaled = (x - x.mean()) / x.std()\n",
    "y_scaled = (y - y.mean()) / y.std()\n",
    "\n",
    "# Ajout de la constante.\n",
    "X_scaled = sm.add_constant(x_scaled)\n",
    "\n",
    "# Ajustement du modèle.\n",
    "model_scaled = sm.OLS(y_scaled, X_scaled).fit()\n",
    "\n",
    "# Prédictions.\n",
    "y_pred = model_scaled.predict(X_scaled)\n",
    "\n",
    "# Afficher le résumé.\n",
    "print(model_scaled.summary())\n",
    "# Erreur quadratique moyenne (MSE).\n",
    "print(\"MSE :\", model_scaled.mse_total)\n",
    "\n",
    "# Tracer les résultats.\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_scaled, y_scaled, color='pink', label='Données observées')\n",
    "plt.plot(x_scaled, y_pred, color='blue', label='Ligne de régression')\n",
    "plt.xlabel('Longueur de la nageoire (mm) standardisée')\n",
    "plt.ylabel('Longueur du bec (mm) standardisée')\n",
    "plt.title('Ligne de régression : Longueur du bec vs Longueur de la nageoire')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027d82e-ffef-4660-be3e-d93877befdc1",
   "metadata": {},
   "source": [
    "- **Le signalement 2 du résumé de statmodels n'apparaît plus, néanmoins le modèle 1 comparait la même unité et affichait tout de même le signalement là où celui-ci n'apparaissait pas dans le modèle 3 malgré deux unités différentes. Ce faisant, les données doivent aussi présenter une très grande variabilité de base. Cela pourrait aussi indiquer que les variables dans le modèle 1 sont plus fortement corrélées que celles du modèle 3.<br><br>**\n",
    "- **Par contre, on remarque que le MSE est passé de 194 à 1 entre le modèle fait précédemment et celui-ci qui est corrigé (mis à l'échelle). Ce faisant, on peut conclure que le problème vu précédemment vis-à-vis de la précision des prédictions établies était dû à l'absence de mise à l'échelle !<br><br>**\n",
    "- **On pourrait (et devrait dans une analyse) vérifier que c'est bien le cas pour la troisième paire de variables mais l'absence de mise à l'échelle était volontaire et à but pédagogique pour expliciter davantage l'utilisation des bibliothèques et les erreurs possibles. Ce faisant, on considérera, la partie régression linéaire comme étant finie. À vous de jouer !<br><br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010430f-8dea-473d-a990-497d49d1dadd",
   "metadata": {},
   "source": [
    "# Principe de l'ACP (Analyse par Composantes Principales) ou PCA en anglais :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f76b63-8fe5-4173-aa84-982b0c4e2639",
   "metadata": {},
   "source": [
    "### Définition :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398d042-a00a-4cac-b3bd-916a79373ea6",
   "metadata": {},
   "source": [
    "L'ACP est une technique en statistique qui permet de réduire les dimensionnalités d'un ensemble de données d'une base à une autre plus petite tout en conservant le maximum d'informations. Elle permet de transformer des données corrélées en un ensemble de variables indépendantes, appelées composantes principales, qui sont ordonnées de façon à expliquer au mieux la\n",
    "variance des données d'origine : la première composante principale capte la plus grande part de variation, la seconde représente la plus grande part restante, et ainsi de suite. Pour choisir le bon nombre de composantes pour l'ACP, il faut prendre une valeur de composantes qui expliquent un pourcentage satisfaisant de la variance totale (80 % par exemple). Pour cela, on peut utiliser la \"méthode du coude\" qui permet d'identifier graphiquement le point à partir duquel le gain de variance n'est pas très important. <br>\n",
    "L'ACP permet ainsi d'analyser plusieurs variables simultanément, et de voir d'éventuelles relations \"cachées\" dans les données, autrement dit, de trouver des regroupements qu'on n'aurait pas forcément vus au premier abord.\\\n",
    "L'ACP est une technique venant de la physique, un exemple simple pour mieux la comprendre est la photographie : une photo transforme une vision en 3D en une photo sur un plan 2D (les photos montrent bien les distances mais mal le relief en général, car elles ont perdu la profondeur comme dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1cfb5c-c3b9-4a2c-aad5-da9c8ae12e4a",
   "metadata": {},
   "source": [
    "## Les différentes étapes pour réaliser une ACP :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4341754-46f5-461e-85fc-77fcedf390ba",
   "metadata": {},
   "source": [
    "- **Étape 1 - Normalisation des variables :** On place les données sur des échelles comparables pour éviter que des variables à grande variance dominent l’analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0593f448-e477-487f-bb47-24aa4435ad89",
   "metadata": {},
   "source": [
    "- **Étape 2 - Création d'une matrice de covariance :** Permet d’identifier les corrélations entre les variables, en calculant une matrice qui montre comment chaque paire de variables est liée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3580053-0ecd-4cd5-b64d-650c0094d3b8",
   "metadata": {},
   "source": [
    "- **Étape 3 - Identification des composantes principales :** Décomposer la matrice  pour trouver les valeurs propres  (quantité de variance expliquée par chaque composante) et vecteurs propres (représentent la direction des composantes principales, qui sont orthogonaux)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cb515-4c28-4310-96e4-e9062749d1f9",
   "metadata": {},
   "source": [
    "- **Étape 4 - Sélection des caractéristiques :** En fonction de l’importance des composantes, on choisit celles à conserver pour constituer un vecteur des caractéristiques, favorisant la réduction de dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2761c-861a-4376-bbb5-87e473e15e50",
   "metadata": {},
   "source": [
    "- **Étape 5 - Projection des données :** Enfin, les données sont projetées sur les axes des composantes principales pour créer un espace réduit qui conserve l’essentiel de l’information.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1a086-65cc-49af-aefc-426c7044dba1",
   "metadata": {},
   "source": [
    "Importation des sous bibilothèques pour scikit-learn et statsmodels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e5741-fcac-4724-8312-52c82d6421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# pour normaliser le jeu de données.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.multivariate.pca import PCA as StatsmodelsPCA\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd49861-1871-433f-bd6f-1e7697a25db0",
   "metadata": {},
   "source": [
    "Ensemble des caractéristiques des différentes variables :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c30335-d13f-41a5-8e92-be3d9f12c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse descriptive du jeu de donées.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1e741-7889-4e37-8667-8fbbc3976d9c",
   "metadata": {},
   "source": [
    "# ACP - code commun aux deux bibliothèques :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9773b-4b29-44a4-91f2-eb4dfb2466b5",
   "metadata": {},
   "source": [
    "## Visualisation de la heatmap de la matrice de corrélation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b870201-96f6-49c4-b780-f3198525cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données.\n",
    "data = pd.read_csv(\"penguins.csv\")\n",
    "\n",
    "# Sélection des variables quantitatives.\n",
    "quantitative_data = data[['bill_length_mm', 'bill_depth_mm', \n",
    "                           'flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "# Imputation des valeurs manquantes avec la moyenne des colonnes.\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "quantitative_data_imputed = imputer.fit_transform(quantitative_data)\n",
    "\n",
    "# Normalisation des données.\n",
    "scaler = StandardScaler()\n",
    "quantitative_data_normalized = scaler.fit_transform(quantitative_data_imputed)\n",
    "\n",
    "# Visualisation de la heatmap de corrélation.\n",
    "correlation_matrix = pd.DataFrame(quantitative_data_normalized,\n",
    "                                  columns=quantitative_data.columns).corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Heatmap de la matrice de corrélation\")\n",
    "plt.imshow(correlation_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "# Ajout des valeurs numériques dans chaque case de la heatmap.\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        plt.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\",\n",
    "                 ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "plt.xticks(range(len(correlation_matrix.columns)),\n",
    "           correlation_matrix.columns, rotation=45)\n",
    "plt.yticks(range(len(correlation_matrix.columns)),\n",
    "           correlation_matrix.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da6e1f-a860-490e-b1da-76ba440e86d6",
   "metadata": {},
   "source": [
    "### Corrélations entre les variables :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5c778-a0db-48ab-a9ea-d9d4adb66d28",
   "metadata": {},
   "source": [
    "La **heatmap de la matrice de corrélation** est une représentation graphique qui illustre les relations entre les différentes variables d’un jeu de données. Chaque cellule de cette matrice est colorée en fonction de la force et du sens de la corrélation entre deux variables, selon une échelle de couleur (ici, rouge pour des corrélations positives et bleu pour des corrélations négatives).\n",
    "\n",
    "Dans ce cas, la **heatmap** met en évidence des relations significatives entre les variables. Par exemple, on observe que les variables *body_mass_g* (masse corporelle) et *flipper_length_mm* (longueur des nageoires) sont fortement et positivement corrélées, indiquant qu'une augmentation de l'une est souvent associée à une augmentation de l'autre. En revanche, d'autres combinaisons de variables montrent des corrélations plus faibles, suggérant une indépendance relative entre elles.\n",
    "\n",
    "Le but de la **heatmap** est de visualiser rapidement ces relations et de détecter des redondances entre les variables. Ces redondances signifient que certaines dimensions des données contiennent des informations similaires, ce qui justifie l’utilisation d’une méthode de réduction de dimensionnalité, comme l’Analyse en Composantes Principales (ACP), pour synthétiser ces relations et simplifier l’analyse tout en conservant l'essentiel des informations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4639836b-37c0-4e7d-8264-c52cd83b4a3f",
   "metadata": {},
   "source": [
    "## Tableau de corrélation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77468c1-c71c-41e7-82a0-fbdc3f14995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du tableau de corrélation brut (avant normalisation).\n",
    "correlation_matrix = quantitative_data.corr()\n",
    "\n",
    "# Affichage du tableau de corrélation.\n",
    "print(\"Tableau de corrélation :\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5854f48-36c5-4fef-8874-104b3b41704d",
   "metadata": {},
   "source": [
    "La fonction *quantitative_data.corr()* génère une matrice de corrélation qui quantifie la relation linéaire entre chaque paire de variables. Les valeurs obtenues vont de -1 (corrélation négative parfaite) à +1 (corrélation positive parfaite), avec 0 indiquant une absence de  corrélation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b5397-d35f-43b4-b76c-9c6adea8f903",
   "metadata": {},
   "source": [
    "# ACP avec scikit-learn :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417e5bc-d1a2-4ed3-9acb-15faa5e6b129",
   "metadata": {},
   "source": [
    "## Variances expliquées et projection de l'ACP :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28620e-78b6-4f78-b0ae-e88382142320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des étiquettes.\n",
    "labels = data['species']\n",
    "\n",
    "# Application de l'ACP avec Scikit-learn.\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(quantitative_data_normalized)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Affichage de la variance expliquée.\n",
    "print(\"Variance expliquée par les composantes principales :\")\n",
    "for i, var in enumerate(explained_variance[:2], 1):\n",
    "    print(f\"Composante {i} : {var:.2%}\")\n",
    "\n",
    "# Affichage des résultats de l'ACP sous forme de graphique simple.\n",
    "plt.figure(figsize=(8, 6))\n",
    "species_colors = labels.map({'Adelie': 'blue', 'Gentoo': 'red', 'Chinstrap': 'green'})\n",
    "scatter = plt.scatter(components[:, 0], components[:, 1], \n",
    "                      c=species_colors, edgecolor=\"k\", s=50)\n",
    "plt.title(\"Projection ACP avec scikit-learn (2 premières composantes)\")\n",
    "plt.xlabel(f\"Composante 1 ({explained_variance[0]:.2%} variance expliquée)\")\n",
    "plt.ylabel(f\"Composante 2 ({explained_variance[1]:.2%} variance expliquée)\")\n",
    "\n",
    "# Ajout de la légende.\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=color, markersize=10, \n",
    "                             label=species)\n",
    "                  for species, color in zip(labels.unique(), ['blue', 'red', 'green'])]\n",
    "plt.legend(handles=legend_handles)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f85a0-ffce-4f53-b8aa-534f23ddbdc4",
   "metadata": {},
   "source": [
    "#### Variance expliquée :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f877a2-ce21-48d5-aebc-cfa985f30be4",
   "metadata": {},
   "source": [
    "- La première composante principale explique environ **68,84 %** de la variance totale des données, ce qui indique qu'elle capte la majorité de l'information. Cela suggère qu'il existe une direction principale dans l'espace des variables où les observations sont fortement dispersées.\n",
    "- La deuxième composante principale explique environ **19,31 %** de la variance totale. Bien qu'elle soit moins informative que la première, elle permet de capturer des variations complémentaires. <br><br>\n",
    "Ensemble, ces deux composantes expliquent **88,15 %** de la variance totale, ce qui signifie qu'une représentation bidimensionnelle est suffisante pour décrire la structure globale des données avec une bonne précision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05412ef5-4f35-43c9-9dee-c452631e25bf",
   "metadata": {},
   "source": [
    "#### Projection  de l'ACP :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64df94d2-e821-4e68-8f57-952321703639",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Le graphique de projection montre les trois espèces de manchots : Adélie (bleu), Gentoo (rouge), et Chinstrap (vert), projetées sur les deux premières composantes principales. Voici l'interprétation :\n",
    "\n",
    "- Adélie (bleu) : Les points bleus sont regroupés principalement dans la partie inférieure gauche du graphique, indiquant une certaine homogénéité au sein de cette espèce sur les deux premières composantes principales.\n",
    "\n",
    "- Chinstrap (vert) : Les points verts se situent légèrement au-dessus des points bleus en les chevauchant partiellement, mais ils sont bien distingués des points rouges (Gentoo). \n",
    "\n",
    "- Gentoo (rouge) : Les points rouges sont concentrés dans la partie droite du graphique, avec une plus grande dispersion sur la composante principale 1. Cela suggère que cette espèce est plus variable que les autres en termes de morphologie, mais reste clairement séparée des autres espèces sur la projection.\n",
    "\n",
    "\n",
    "En résumé, le graphique montre une séparation claire entre les Gentoo et les deux autres espèces (Adélie et Chinstrap),qui présentent un chevauchement entre elles. Cela suggère que, bien que certaines caractéristiques morphologiques puissent différencier les espèces,  Adélie et Chinstrap partagent des caractéristiques similaires sur les deux premières composantes principales, tandis que Gentoo forme un groupe distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070e4d7-eaaf-4640-a03a-e7b9dd2f83e4",
   "metadata": {},
   "source": [
    "#### Interprétation des résultats :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f15fb-5064-440d-a864-a1f168c7a76c",
   "metadata": {},
   "source": [
    "La première composante semble être associée à une variation générale de taille corporelle (variables comme *\"body_mass_g\"* et *\"flipper_length_mm\"*), capturant des différences globales entre les espèces.\n",
    "La deuxième composante pourrait refléter des variations plus subtiles liées à des proportions ou des caractéristiques spécifiques comme la relation entre la longueur et la profondeur du bec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71ddff-6d0e-405c-a8fd-f68cb75a3969",
   "metadata": {},
   "source": [
    "## ACP avec statsmodels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646e6c4-f254-43d3-a31a-c6d5e77e8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données.\n",
    "data = pd.read_csv(\"penguins.csv\")\n",
    "\n",
    "# Sélection des variables quantitatives et des étiquettes.\n",
    "quantitative_data = data[['bill_length_mm', 'bill_depth_mm', \n",
    "                           'flipper_length_mm', 'body_mass_g']]\n",
    "labels = data['species']\n",
    "\n",
    "# Imputation des valeurs manquantes avec la moyenne des colonnes.\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "quantitative_data_imputed = imputer.fit_transform(quantitative_data)\n",
    "\n",
    "# Normalisation des données.\n",
    "scaler = StandardScaler()\n",
    "quantitative_data_normalized = scaler.fit_transform(quantitative_data_imputed)\n",
    "\n",
    "# Application de l'ACP avec Statsmodels.\n",
    "# Statsmodels PCA nécessite des données normalisées.\n",
    "pca = sm.multivariate.PCA(quantitative_data_normalized, ncomp=2)\n",
    "\n",
    "# Extraction des résultats de l'ACP.\n",
    "# Composantes principales.\n",
    "pca_components = pca.factors\n",
    "# Variance expliquée par chaque composante.\n",
    "explained_variance = pca.eigenvals / sum(pca.eigenvals)\n",
    "\n",
    "# Affichage de la variance expliquée.\n",
    "print(\"Variance expliquée par les composantes principales :\")\n",
    "for i, var in enumerate(explained_variance, 1):\n",
    "    print(f\"Composante {i} : {var:.2%}\")\n",
    "\n",
    "# Visualisation de la projection des données sur les 2 premières composantes.\n",
    "plt.figure(figsize=(8, 6))\n",
    "species_colors = labels.map({'Adelie': 'blue', 'Gentoo': 'red', 'Chinstrap': 'green'})\n",
    "plt.scatter(pca_components[:, 0], pca_components[:, 1], \n",
    "            c=species_colors, edgecolor=\"k\", s=50)\n",
    "\n",
    "# Titres et étiquettes.\n",
    "plt.title(\"Projection ACP avec statsmodels (2 premières composantes)\")\n",
    "plt.xlabel(f\"Composante 1 ({explained_variance[0]:.2%} variance expliquée)\")\n",
    "plt.ylabel(f\"Composante 2 ({explained_variance[1]:.2%} variance expliquée)\")\n",
    "\n",
    "# Ajout de la légende.\n",
    "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=color, markersize=10, \n",
    "                             label=species)\n",
    "                  for species, color in zip(labels.unique(), ['blue', 'red', 'green'])]\n",
    "plt.legend(handles=legend_handles)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb39466-21b5-405b-a2b3-52f12b0d5b92",
   "metadata": {},
   "source": [
    "#### Projection  de l'ACP :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03846f9-274b-4694-ba3e-1aa999f9008c",
   "metadata": {},
   "source": [
    "Sur le graphique ci-dessus, on peut voir :\n",
    "- Les points rouges (Gentoo) qui sont séparés des deux autres groupes et sont bien distants sur l'axe de la composante 1. Cette séparation nette des Gentoo suggère que cette espèce a des caractéristiques morphologiques distinctes qui permettent une identification facile par l'ACP.\n",
    "\n",
    "- Les points bleus (Adelie) et verts (Chinstrap) montrent un chevauchement important, particulièrement sur la composante 1 (axe horizontal). Cela indique que ces deux espèces partagent des caractéristiques similaires et que l'ACP ne les sépare pas aussi efficacement que les Gentoo.\n",
    "\n",
    "Le chevauchement des groupes bleus et verts indique que la différence entre ces deux espèces est plus subtile et qu'il pourrait y avoir un chevauchement plus marqué dans les traits morphologiques. Cela pourrait être dû à une variabilité plus faible au sein de ces espèces, ou à des traits morphologiques qui se ressemblent davantage entre elles comparées aux Gentoo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a5194-ee24-45a0-8b81-f83e21f68fd9",
   "metadata": {},
   "source": [
    "## Comparaison entre les deux bibliothèques :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335eebc-a796-42f5-91b6-a518706e050d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Avec **statsmodels**, nous avons obtenu une projection des données sur les deux premières composantes principales, qui expliquent respectivement **78,09 %** et **21,91 %** de la variance. Le graphique issu de cette analyse montre une séparation nette des groupes, en particulier pour les Gentoo, qui sont distinctement séparés des autres groupes. Cependant, Adelie (en bleu) et Chinstrap (en vert) présentent un chevauchement important, ce qui indique que ces deux espèces partagent des caractéristiques morphologiques similaires, rendant leur séparation plus difficile.<br>\n",
    "Ces résultats suggèrent que la méthode peut être efficace pour réduire la dimensionnalité et repérer des distinctions marquées (comme pour les Gentoo), mais elle peut aussi avoir des limitations lorsqu'il s'agit de différencier des groupes qui se chevauchent ou sont plus proches dans l’espace des caractéristiques (Adelie et Chinstrap).\n",
    "\n",
    "- Avec **scikit-learn**, l'ACP nous a permis de visualiser la répartition des espèces sur les deux premières composantes principales, où la composante 1 expliquait **68,84 %** de la variance et la composante 2 **19,31 %**. Le graphique résultant est assez similaire à celui de **statsmodels**, mais avec une légère différence d’orientation des groupes. Les Gentoo (rouges) sont de nouveau bien séparés, tandis que les Adélie et Chinstrap se chevauchent, tout comme dans l'analyse avec **statsmodels**. La différence principale réside dans l’orientation du graphique, mais cette variation ne change pas fondamentalement la conclusion sur les relations entre les espèces.\n",
    "\n",
    "    - **Séparation des groupes** : Les deux bibliothèques montrent une bonne séparation des Gentoo (en rouge) des autres espèces, ce qui suggère que ces manchots possèdent des caractéristiques morphologiques suffisamment distinctes pour être identifiés facilement dans le cadre de cette analyse. Cependant, Adelie et Chinstrap se chevauchent dans les deux analyses, ce qui peut indiquer que ces deux espèces partagent des traits physiques similaires qui les rendent difficiles à distinguer sur les premières composantes principales.\n",
    "\n",
    "    - **Différences dans l’orientation** : La principale différence entre les résultats des deux bibliothèques réside dans l'orientation des groupes. **Statsmodels** semble offrir une projection où les groupes sont légèrement inclinés différemment par rapport à **scikit-learn**, ce qui pourrait être dû à la manière dont les composantes principales sont extraites ou à des différences dans les paramètres par défaut des deux méthodes. En effet, normalement peu importe la méthode de calcul utilisée pour les ACP, les valeurs propres seront toujours les mêmes mais les vecteurs propres, eux, peuvent être différents. Comme ceux-ci déterminent l'orientation selon les axes des graphiques, des différences dans la base à deux dimensions peuvent apparaître (différences d'orientation). Mais peu importe l'orientation du graphique, les distances entre les clusters (groupes dans les données) seront les mêmes (penser à la valeur absolue en mathématiques). Ainsi, deux ACP peuvent donner deux graphiques différents qui disent exactement la même chose, il faut donc être vigilant !\n",
    "\n",
    "    - **Précision des résultats** : Les deux méthodes fournissent des résultats similaires en termes de variance expliquée et de répartition des groupes, bien que **scikit-learn** soit souvent préféré dans le cadre d’analyses d’ACP en raison de sa flexibilité, de sa vitesse et de ses outils de visualisation plus puissants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf332a-70ca-46ba-9765-9932921ce519",
   "metadata": {},
   "source": [
    "## Conclusion générale sur l'ACP :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83d694-a010-48f3-8e8c-74ed8352e00b",
   "metadata": {},
   "source": [
    "Qu’elle soit réalisée avec **statsmodels** ou **scikit-learn**, l'ACP nous offre une vision claire de la structure des données et nous aide à comprendre les relations entre les différentes espèces de manchots. L'ACP a réussi à réduire la complexité des données tout en préservant leur structure essentielle. La distinction entre les espèces sur la projection bidimensionnelle valide la pertinence des variables choisies pour cette analyse. Les deux bibliothèques montrent que les Gentoo peuvent être facilement distingués des autres espèces, mais que les Adélie et Chinstrap se chevauchent, suggérant qu'il peut être difficile de les différencier uniquement à partir des variables mesurées (longueur et profondeur du bec, longueur des nageoires, masse corporelle).\n",
    "\n",
    "En résumé, **scikit-learn** et **statsmodels** sont deux bibliothèques efficaces pour effectuer une ACP, présentant des petites différences dans les résultats, notamment dans l'orientation des projections. **Scikit-learn** est davantage utilisé dans le milieu de l'analyse de données en raison de sa grande souplesse et de sa simplicité d'utilisation, tandis que **statsmodels** peut offrir des résultats similaires, mais avec une approche davantage axée sur la statistique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415927a-ed9b-4a89-80fa-f8de3ba24e24",
   "metadata": {},
   "source": [
    "## Autre possibilité de représentation graphique pour l'ACP (avec statsmodels) :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce4d0c2-88a4-4238-a238-1483e5f8d82d",
   "metadata": {},
   "source": [
    "Il est également possible de visualiser une ACP sous forme de **cercle de corrélation** en utilisant une méthode appelée la décomposition en valeurs propres. Cette analyse est effectuée après avoir sélectionné uniquement les variables numériques (telles que la longueur et la profondeur du bec, la longueur des nageoires, et la masse corporelle des manchots). L'objectif principal est de comprendre les relations entre ces différentes variables à l’aide de la matrice de corrélation, qui quantifie les liens linéaires entre elles. En effectuant la décomposition en valeurs propres de cette matrice, nous obtenons des vecteurs propres (ou directions principales), qui sont ensuite visualisés à l’aide d’un cercle des corrélations, permettant d’illustrer graphiquement l’orientation et l’importance des relations entre les variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9d370-7235-435c-99f9-f48216e3d0ed",
   "metadata": {},
   "source": [
    "#### Cercle de corrélation juste avec statsmodels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b4330-6ce1-4b19-8a3a-63a4059106f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner uniquement les colonnes numériques.\n",
    "quantitative_data = data[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "# Calculer la matrice de corrélation des variables.\n",
    "corr_matrix = quantitative_data.corr()\n",
    "\n",
    "# Effectuer la décomposition en valeurs propres.\n",
    "eigvals, eigvecs = np.linalg.eig(corr_matrix)\n",
    "\n",
    "# Trier les valeurs propres et les vecteurs propres par ordre décroissant.\n",
    "sorted_indices = np.argsort(eigvals)[::-1]\n",
    "eigvals = eigvals[sorted_indices]\n",
    "eigvecs = eigvecs[:, sorted_indices]\n",
    "\n",
    "# Créer un graphique de cercle des corrélations avec des couleurs et annotations.\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "\n",
    "# Tracer le cercle.\n",
    "circle = plt.Circle((0, 0),\n",
    "                    1,\n",
    "                    color='gray',\n",
    "                    fill=False,\n",
    "                    linestyle='--')\n",
    "ax.add_artist(circle)\n",
    "\n",
    "# Couleurs pour chaque vecteur propre (pour 4 variables).\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "\n",
    "# Tracer les vecteurs propres (les directions principales) avec couleurs et annotations.\n",
    "quantitative_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "# Utiliser uniquement les colonnes quantitatives.\n",
    "for i, var in enumerate(quantitative_columns):\n",
    "    # Vérification de la dimension pour éviter l'IndexError.\n",
    "    if i < eigvecs.shape[1]:\n",
    "        ax.quiver(0, 0,\n",
    "                  eigvecs[0, i],\n",
    "                  eigvecs[1, i],\n",
    "                  angles='xy',\n",
    "                  scale_units='xy',\n",
    "                  scale=1,\n",
    "                  color=colors[i],\n",
    "                  label=f'{var} (PC {i+1})')\n",
    "        ax.text(eigvecs[0, i] * 1.1,\n",
    "                eigvecs[1, i] * 1.1,\n",
    "                var,  # Ajouter le nom de la variable ici.\n",
    "                color=colors[i],\n",
    "                fontsize=12,\n",
    "                ha='center')\n",
    "\n",
    "# Ajouter des labels et un titre.\n",
    "ax.set_xlabel('Composante 1')\n",
    "ax.set_ylabel('Composante 2')\n",
    "ax.set_title('Cercle des corrélations avec statsmodels')\n",
    "\n",
    "# Afficher la légende.\n",
    "ax.legend()\n",
    "\n",
    "# Afficher le graphique.\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "# Afficher les valeurs propres et les composantes principales.\n",
    "print(\"Valeurs propres (explained variance) :\")\n",
    "print(eigvals)\n",
    "\n",
    "print(\"\\nVecteurs propres (composantes principales) :\")\n",
    "print(eigvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e247712-0a56-44c1-8c50-2ff47e958df5",
   "metadata": {},
   "source": [
    "Le code effectue les étapes suivantes :\n",
    "\n",
    "- Calcule la matrice de corrélation des variables quantitatives.\n",
    "- Effectue la décomposition en valeurs propres de cette matrice.\n",
    "- Trie les valeurs propres et les vecteurs propres par ordre décroissant, afin d'identifier les directions principales de variance.\n",
    "- Génère un cercle des corrélations sur un graphique pour visualiser ces vecteurs propres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22514729-2442-48cd-b948-de0b16da854b",
   "metadata": {},
   "source": [
    "### Analyse des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57338844-d6e6-417f-97b9-bbb2bbf8a074",
   "metadata": {},
   "source": [
    "Le **cercle des corrélations** obtenu à partir de la décomposition en valeurs propres offre une vue visuelle de l’importance des relations entre les variables et de la direction de la plus grande variance dans les données. Dans ce graphique, les vecteurs propres représentent les directions dans lesquelles les variables contribuent le plus à la variance. Chaque vecteur est associé à une variable spécifique, et les directions des vecteurs indiquent la façon dont chaque variable est liée aux autres.\n",
    "\n",
    "**Vecteurs propres (Composantes Principales) :**\n",
    "- *Vecteurs propres* : Ce sont des directions principales (composantes principales) dans lesquelles les données varient le plus. Chaque vecteur propre est un axe dans un espace multidimensionnel. Plus l'angle entre un vecteur propre et une variable est petit, plus la variable est fortement corrélée avec cette direction. En d'autres termes, les variables qui pointent dans des directions similaires sont plus fortement corrélées entre elles.\n",
    "- *Valeurs propres* : Elles représentent la variance expliquée par chaque composante principale. Plus une valeur propre est grande, plus la composante principale associée capture la variance des données. Si une composante a une valeur propre élevée, cela signifie qu'elle contient une proportion importante de l'information des données d'origine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e55f1-7983-4082-a3c2-0b961340064b",
   "metadata": {},
   "source": [
    "#### Interprétation des vecteurs dans le cercle des corrélations :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a1322-486c-40f4-be34-e6cca8c68d05",
   "metadata": {},
   "source": [
    "- Les vecteurs proches du bord du cercle (indiquant une direction forte de la variance) suggèrent que les variables sont fortement liées entre elles dans ces directions. Par exemple, si deux vecteurs sont proches dans le graphique, cela signifie que ces deux variables sont fortement corrélées.\n",
    "- Les angles entre les vecteurs peuvent aussi donner des indices sur la corrélation entre les variables. Si deux vecteurs sont orientés dans des directions opposées (un angle de 180 degrés), les variables qu'ils représentent sont inversement corrélées. Si les vecteurs sont presque parallèles, les variables sont positivement corrélées.\n",
    "- Si un vecteur est proche du centre du cercle, cela signifie que la variable associée à ce vecteur a une faible contribution à la variance expliquée et que sa relation avec les autres variables est moins forte. <br>\n",
    "\n",
    "**Exemple**  :\n",
    "Les vecteurs représentant *\"flipper_length_mm\"* et *\"body_mass_g\"* sont proches et pointent dans des directions similaires, cela indique que ces deux variables sont fortement corrélées. Ainsi, une variation dans la longueur du bec pourrait être associée à une variation similaire avec la masse corporelle. En revanche, le vecteur qui représente *\"bill_length_mm\"* est éloigné des autres vecteurs, cela suggère que cette variable contribue de manière unique à la variance des données, moins corrélée aux autres mesures morphologiques. De plus, en regardant le cercle des corrélations, on observe que les vecteurs de *\"bill_length_mm\"* et *\"bill_depth_mm\"* semblent être presque orthogonaux (c'est-à-dire perpendiculaires), ce qui suggère une faible ou aucune corrélation. En effet, dans la heatmap matrice de corrélation, on voit une valeur de -0.24, ce qui montre que ces caractéristiques ne sont pas corrélées.\n",
    "\n",
    "En résumé, ce graphique de cercle des corrélations permet de comprendre rapidement quelles variables sont les plus liées et d'identifier les axes principaux de variance dans les données, tout en offrant une représentation visuelle claire des relations multidimensionnelles entre les différentes variables étudiées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1feacf-0a56-453a-9523-8a435ae9cfe1",
   "metadata": {},
   "source": [
    "# Conclusion de la comparaison des bibliothèques pour réaliser des régressions linéaires et des ACP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6630537-6a3f-4363-9d75-e3f4f0272fbc",
   "metadata": {},
   "source": [
    "- Au cours de ce tutoriel, nous avons comparé les bibliothèques **scikit-learn** et **statsmodels** lors de la réalisation de deux analyses statistiques principales :  \n",
    "  - La régression linéaire  \n",
    "  - L'Analyse en Composantes Principales (ACP)<br><br>   \n",
    "\n",
    "- Il en ressort que les deux bibliothèques sont similaires et permettent de réaliser ces deux analyses de manière claire, efficace et comparable. Malgré des différences dans la préparation des données, les affichages et les méthodes de calcul, **les résultats finaux sont identiques**.  \n",
    "\n",
    "- À noter et comme vu précédemment :  \n",
    "  - **Statsmodels** est une bibliothèque dédiée aux statistiques \"classiques\", orientée vers la robustesse statistique et la vérification d’hypothèses.  \n",
    "  - **Scikit-learn**, quant à elle, est davantage orientée vers le machine-learning (apprentissage automatique) et privilégie l’optimisation des performances.  \n",
    "\n",
    "Ce faisant, les quelques différences notables proviennent majoritairement de ces différences :  \n",
    "1. Pour la régression linéaire, **statsmodels** semblait plus robuste, mettant en avant des indicateurs statistiques détaillés, tandis que **scikit-learn** nécessitait un format de données particulier pour optimiser ses performances.  \n",
    "2. Pour l’ACP, les deux bibliothèques donnaient des résultats identiques et des conclusions similaires, bien que leurs interfaces et options diffèrent légèrement.  \n",
    "\n",
    "En conclusion, le choix entre ces bibliothèques dépend de vos besoins : **validation statistique rigoureuse** (statsmodels) ou **optimisation pour le machine-learning** (scikit-learn).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
